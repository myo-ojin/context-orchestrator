# Context Orchestrator (å¤–éƒ¨è„³ã‚·ã‚¹ãƒ†ãƒ )

An external brain system that acts as an MCP (Model Context Protocol) server, enabling developers to capture, organize, and retrieve their work experiences across any LLM client.

## Features

### Core Capabilities

- ğŸ§  **Automatic Memory Capture**: Transparently records CLI conversations (Claude, Codex)
- ğŸ“Š **Schema Classification**: Organizes memories into Incident, Snippet, Decision, Process
- ğŸ” **Hybrid Search**: Vector (semantic) + BM25 (keyword) search with intelligent reranking
- ğŸ  **Local-First Privacy**: Embeddings and classification run locally (Ollama)
- âš¡ **Smart Model Routing**: Light tasks â†’ local LLM, heavy tasks â†’ cloud LLM
- ğŸ’¾ **Memory Hierarchy**: Working â†’ Short-term â†’ Long-term memory like human brain
- ğŸŒ™ **Auto Consolidation**: Nightly memory consolidation and forgetting

### Integrations

- ğŸ““ **Obsidian Integration**: Auto-detect and ingest conversation notes from Obsidian vault
  - Monitors `.md` files for conversation patterns
  - Extracts Wikilinks for relationship tracking
  - Parses YAML frontmatter (tags, metadata)
- ğŸ“ **Session Logging**: Preserves full terminal transcripts with auto-summarization
- ğŸ”Œ **MCP Protocol**: Works with any MCP-compatible client (Claude CLI, Cursor, VS Code)

### Management Tools

- ğŸ“Š **System Status**: Comprehensive health monitoring with `status` command
- ğŸ©º **Diagnostics**: Automated troubleshooting with `doctor` command
- ğŸ’¾ **Backup/Restore**: Export and import memories with `export`/`import` commands
- ğŸ“‹ **Session History**: View and manage session logs and summaries

## Quick Start

### Prerequisites

- Python 3.11+
- Ollama (for local LLM)
- PowerShell (for Windows CLI integration)
- (Optional) chromadb `pip install chromadb` ? required if you want to run vector DB integration tests

### Installation

```bash
# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Run setup wizard
python scripts/setup.py
```

### Download Required Models

```bash
ollama pull nomic-embed-text
ollama pull qwen2.5:7b
```

## Usage

### Start MCP Server
```bash
# Start Context Orchestrator as MCP server
python -m src.main

# Or use console entry point (if installed)
context-orchestrator
```

### CLI Commands

```bash
# System status and health
python -m src.cli status      # Show comprehensive system status
python -m src.cli doctor      # Run diagnostics

# Memory management
python -m src.cli consolidate         # Manual memory consolidation
python -m src.cli list-recent --limit 20  # List recent memories
python -m src.cli export --output backup.json  # Export memories
python -m src.cli import --input backup.json   # Import memories

# Session history
python -m src.cli session-history  # List all sessions
python -m src.cli session-history --session-id <id>  # Show specific session

# Performance profiling
python scripts/performance_profiler.py  # Run performance benchmarks
```

See [CLAUDE.md](CLAUDE.md) for detailed CLI documentation.

### Structured Summaries & Scenario Loader

Every ingested conversation must produce a structured summary with the following exact layout:

```
Topic: <short topic name>
DocType: <incident|decision|checklist|guide|...>
Project: <project name or Unknown>
KeyActions:
- <Imperative Action 1>
- <Action 2>
```

- `KeyActions` ã¯å¿…ãš `- ` ã§å§‹ã¾ã‚‹ç®‡æ¡æ›¸ãã«ã™ã‚‹ã€‚æ®µè½ã‚„ç•ªå·ä»˜ããƒªã‚¹ãƒˆã¯æ¤œè¨¼ã«å¤±æ•—ã™ã‚‹ã€‚
- `scripts.load_scenarios` ã¯å–ã‚Šè¾¼ã¿æ™‚ã«ã“ã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€é•åãŒã‚ã‚‹ã¨ãƒ¡ãƒ¢ ID ã¨ç”Ÿæˆã‚µãƒãƒªã®æŠœç²‹ã‚’è¡¨ç¤ºã—ã¦ä¸­æ–­ã™ã‚‹ã€‚
- ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯è©²å½“ä¼šè©±ã‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä¿®æ­£ã—ã€`python -m scripts.load_scenarios --file tests/scenarios/scenario_data.json` ã‚’å†å®Ÿè¡Œã™ã‚‹ã€‚

CI ã® `python -m scripts.run_regression_ci` ã‚‚åŒã˜æ¤œè¨¼ã‚’è¡Œã†ãŸã‚ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ›´æ–°ã—ãŸéš›ã¯ README ã¨ã‚·ãƒŠãƒªã‚ª README ã‚’åŒæœŸã•ã›ã¦ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚’æµã—ã¦ãã ã•ã„ã€‚

## Configuration

Create `~/.context-orchestrator/config.yaml`:

```yaml
# Data storage
data_dir: ~/.context-orchestrator

# Obsidian integration (optional)
obsidian_vault_path: C:\Users\username\Documents\ObsidianVault

# Ollama settings
ollama:
  url: http://localhost:11434
  embedding_model: nomic-embed-text
  inference_model: qwen2.5:7b

# CLI LLM for complex tasks
cli:
  command: claude  # or "codex"

# Search parameters
search:
  candidate_count: 50
  result_count: 10
  timeout_seconds: 2
  cross_encoder_enabled: true
  cross_encoder_top_k: 3
  cross_encoder_cache_size: 128
  cross_encoder_cache_ttl_seconds: 900
  vector_candidate_count: 100
  bm25_candidate_count: 30
  query_attribute_min_confidence: 0.4
  query_attribute_llm_enabled: true

# Memory management
clustering:
  similarity_threshold: 0.9
  min_cluster_size: 2

forgetting:
  age_threshold_days: 30
  importance_threshold: 0.3
  compression_enabled: true

working_memory:
  retention_hours: 8
  auto_consolidate: true

# Consolidation schedule (cron format)
consolidation:
  schedule: "0 3 * * *"  # 3:00 AM daily
  auto_enabled: true

# Session logging
logging:
  session_log_dir: ~/.context-orchestrator/logs
  max_log_size_mb: 10
  summary_model: qwen2.5:7b
  level: INFO

# Language routing (local LLM handles these language codes; others fall back to cloud)
languages:
  supported_local:
    - en
    - ja
    - es
  fallback_strategy: cloud
```

`languages.supported_local` ã«å«ã¾ã‚Œãªã„è¨€èªãŒæ¤œçŸ¥ã•ã‚Œã‚‹ã¨ã€`fallback_strategy` ã«å¾“ã£ã¦ã‚¯ãƒ©ã‚¦ãƒ‰ LLM ã¸ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚çŸ­æœŸçš„ã«ç‰¹å®šè¨€èªã‚’å¼·åˆ¶ã—ãŸã„å ´åˆã¯ MCP ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹ã‚·ã‚§ãƒ«ã§ç’°å¢ƒå¤‰æ•° `CONTEXT_ORCHESTRATOR_LANG_OVERRIDE` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚

```powershell
$env:CONTEXT_ORCHESTRATOR_LANG_OVERRIDE = "fr"
python -m src.main  # ä»¥é™ã®è¦ç´„ã¯ãƒ•ãƒ©ãƒ³ã‚¹èªæ‰±ã„ã§ routing
```

### Cross-Encoder Reranker Cache

- `search.cross_encoder_cache_size` / `search.cross_encoder_cache_ttl_seconds` ã§ LRU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®¹é‡ã¨ä¿æŒæœŸé–“ï¼ˆç§’ï¼‰ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚
- `python -m scripts.mcp_replay` ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚„ LLM ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ â€œReranker Metricsâ€ ã¨ã—ã¦è¡¨ç¤ºã•ã‚Œã€`reports/mcp_runs/*.jsonl` ã«ã‚‚ä¿å­˜ã•ã‚Œã¾ã™ã€‚
- MCP çµŒç”±ã§ `{"jsonrpc":"2.0","id":1,"method":"get_reranker_metrics","params":{}}` ã‚’å‘¼ã³å‡ºã™ã¨ã€ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§å–å¾—ã§ãã¾ã™ã€‚
- `--export-features <path>` ã‚’ä»˜ã‘ã¦ãƒªãƒ—ãƒ¬ã‚¤ã™ã‚‹ã¨ã€å„æ¤œç´¢çµæœã® rerank ç‰¹å¾´é‡ãŒ CSV ã«å‡ºåŠ›ã•ã‚Œã€å¾Œè¿°ã®é‡ã¿å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«æ¸¡ã›ã¾ã™ã€‚

### Rerank Weight Training

1. `python -m scripts.mcp_replay --requests tests/scenarios/query_runs.json --export-features reports/features.csv`
2. `python -m scripts.train_rerank_weights --features reports/features.csv --config config.yaml`
3. `python -m scripts.run_regression_ci` ã‚’å†å®Ÿè¡Œã—ã¦ Precision/NDCG ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ `reranking_weights` ã‚„ `search.cross_encoder_cache_*` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ã‚¯ãƒ©ã‚¦ãƒ‰å´ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã™ã‚‹ã¨ `Language routing fallback (lang=...)` ãƒ­ã‚°ã«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆãƒŸãƒªç§’ï¼‰ã¨æˆå¦ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚`python -m scripts.run_regression_ci` ã‚„å¹³å¸¸é‹ç”¨ä¸­ã« `logs/context_orchestrator.log` ã‚’ tail ã—ã¦ãŠã‘ã°ã€é…å»¶ã‚„å¤±æ•—å›æ•°ã‚’ç¶™ç¶šçš„ã«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã™ã€‚


## Troubleshooting

### Ollama Connection Issues

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Verify models are installed
ollama list
```

### PowerShell Wrapper Not Working

```powershell
# Check if wrapper is loaded
Get-Command claude

# Reload profile
. $PROFILE

# Re-run setup
python scripts/setup.py --repair
```

### Search Returns No Results

```bash
# Check if memories are indexed
python -m src.cli list-recent

# Verify database exists
ls ~/.context-orchestrator/chroma_db/

# Check logs
python -m src.cli status

# Run diagnostics
python -m src.cli doctor
```

### High Memory Usage

```bash
# Check consolidation status
python -m src.cli status

# Run manual consolidation
python -m src.cli consolidate

# Export and prune old memories
python -m src.cli export --output backup_$(date +%Y%m%d).json
```

## How It Works

1. **Capture**: PowerShell wrapper intercepts CLI commands and sends conversations to the orchestrator
2. **Classify**: Local LLM classifies memories into schemas (Incident/Snippet/Decision/Process)
3. **Chunk**: Content is split into 512-token chunks for efficient processing
4. **Index**: Chunks are indexed in both Vector DB (semantic) and BM25 (keyword)
5. **Search**: Hybrid search retrieves relevant memories and reranks by importance
6. **Consolidate**: Nightly job migrates working memory, clusters similar memories, and forgets old data

## Architecture

See [CLAUDE.md](CLAUDE.md) for detailed architecture and development guide.

## Performance Targets

- **Search Latency**: 80-200ms (typical: ~80ms)
- **Ingestion Time**: <5 seconds per conversation
- **Memory Footprint**: 1GB resident, 3GB peak (during inference)
- **Disk Usage**: ~10MB/year (~100MB/10 years)
- **Consolidation**: Complete in <5 minutes for 10K memories

### Performance Profiling

Run performance benchmarks to validate system performance:

```bash
# Run all benchmarks
python scripts/performance_profiler.py

# Custom run count
python scripts/performance_profiler.py --runs 200

# Save report to custom location
python scripts/performance_profiler.py --output ./perf_report.json
```

The profiler measures:
- **Search Latency**: P50/P95/P99 latencies with target â‰¤200ms
- **Ingestion Throughput**: Conversations per second, target <5s/conversation
- **Consolidation Time**: Extrapolated time for 10K memories, target <5 minutes
- **Memory Footprint**: Peak and resident memory usage

Reports are saved as JSON with pass/fail indicators for each target.

## Documentation

- **Requirements**: `.kiro/specs/dev-knowledge-orchestrator/requirements.md` - Full project requirements
- **Design**: `designtt.txt` - Detailed architecture and interfaces
- **Tasks**: `.kiro/specs/dev-knowledge-orchestrator/tasks.md` - Implementation roadmap
- **Developer Guide**: `CLAUDE.md` - Development and contribution guidelines
- **Integration Tests**: `INTEGRATION_TEST_RESULTS.md` - Test results and validation

## Project Status

- âœ… **Phase 1-10**: Core system (MCP server, storage, processing, services) - **COMPLETE**
- âœ… **Phase 11**: Obsidian Integration - **COMPLETE**
- âœ… **Phase 12**: CLI Interface - **COMPLETE**
- âœ… **Phase 13**: Testing and Documentation - **COMPLETE**
- âœ… **Phase 14**: Integration & Optimization - **COMPLETE**
  - End-to-end validation tests
  - Performance profiling tool
  - Enhanced error handling and structured logging

**Current Status**: Production Ready

## License

TBD

## Contributing

We welcome contributions! Please see:
- [CLAUDE.md](CLAUDE.md) for coding guidelines and development setup
- [CONTRIBUTING.md](CONTRIBUTING.md) for contribution process (coming soon)

### Development Setup

```bash
# Clone repository
git clone https://github.com/myo-ojin/llm-brain.git
cd llm-brain

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # or .\.venv\Scripts\Activate.ps1 on Windows

# Install dependencies
pip install -r requirements.txt

# Install dev dependencies
pip install pytest pytest-cov black ruff mypy

# Run tests
pytest

# Format code
black .

# Lint
ruff .
```

### Regression Replay Check

Run this guard whenever retrieval, QAM, or memory code changes:

```bash
python -m scripts.run_regression_ci
```

This helper wraps `scripts.mcp_replay` against the canonical baseline (`reports/baselines/mcp_run-20251109-143546.jsonl`), saves the latest log under `reports/mcp_runs/`, and fails if either condition is met:

- Macro Precision or Macro NDCG drops by more than 0.02 versus the baseline.
- `reports/mcp_runs/zero_hits.json` records any zero-hit queries (indicates missing dictionary/metadata entries).

Override `--baseline`, `--requests`, or `--output` when adding new scenarios, and commit refreshed baselines once metrics improve. For CI, activate `.venv311` then add a step such as `python -m scripts.run_regression_ci`; no extra services are required because the script launches the MCP server via `scripts.mcp_stdio`.

## Support

- **Issues**: [GitHub Issues](https://github.com/myo-ojin/llm-brain/issues)
- **Documentation**: [CLAUDE.md](CLAUDE.md)
- **Test Results**: [INTEGRATION_TEST_RESULTS.md](INTEGRATION_TEST_RESULTS.md)
